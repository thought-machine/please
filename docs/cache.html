
    <h1>Please cache</h1>

    <p>In various places in these docs you might find reference to caches.
      Please can make use of several caches to speed up its performance which are described here.</p>

    <p>In all cases artifacts are only stored in the cache after a successful build or test run.<br/>
      Please takes a <code>--nocache</code> flag which disables all caches for an individual run.</p>

    <h2>The directory cache</h2>

    <p>This is the simplest kind of cache; it's on by default and simply is a directory tree
      (by default <code>~/.cache/please</code> or <code>~/Library/Caches/please</code>)
      containing various versions of built artifacts. The main advantage of this is that it allows
      extremely fast rebuilds when swapping between different versions of code
      (notably git branches).</p>

    <p>Note that the dir cache is <b>not</b> threadsafe or locked in any way beyond plz's normal
      repo lock, so sharing the same directory between multiple projects is probably a Bad Idea.</p>

    <h2>The HTTP cache</h2>

    <p>This is a more advanced cache which, as one would expect, can run on a centralised machine
      to share artifacts between multiple clients. It has a simple API based on PUT and GET to
      store and retrieve opaque blobs.</p>

    <p>It is simply configured by setting the <code>httpurl</code> property in the
      <a href="config.html#cache">cache section of the config</a>. There are a couple more settings
      to configure it for readonly mode and to set timeouts etc.</p>

    <p>Since the API is simple there are many existing servers that can be configured for a
      backend; one option is nginx using its
      <a href="http://nginx.org/en/docs/http/ngx_http_dav_module.html">webDAV</a> module.<br/>
      Alternatively some CI services (for example <a href="https://cirrus-ci.org">Cirrus</a>)
      may offer a compatible cache out of the box.</p>

    <p>Thanks to Diana Costea who implemented the original version of this as part of her internship
      with us, and prodded us into getting on and actually deploying it for our CI servers.</p>
